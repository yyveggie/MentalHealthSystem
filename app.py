import rootutils
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)

import asyncio
import json
import operator
import uuid
from bson import ObjectId
from textwrap import dedent
from datetime import datetime
from typing import Optional, Union, List, Dict, Type, TypedDict, Annotated, Sequence, Tuple

import faiss
import websockets
from colorama import Fore, Style
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.callbacks import CallbackManagerForToolRun
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, FunctionMessage, AIMessage
# from langchain_core.pydantic_v1 import BaseModel, Field
from pydantic import BaseModel, Field
from langchain_core.tools import BaseTool
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_ollama import ChatOllama, OllamaLLM, OllamaEmbeddings
from langchain.memory import VectorStoreRetrieverMemory
from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import ToolNode, ToolInvocation, ToolExecutor

from tools import summarize, web_search
from rag.knowledge_graph import retrieve
from memory import explicit_memory, implicit_memory, memory_retrieve
from prompts import guided_conversation, main_system

from load_config import CHAT_MODEL, API_KEY, EMBEDDING_MODEL, EMBEDDING_DIMENSION
from logging_config import setup_logging, disable_logging
import logging
from business.diagnose import MedicalDiagnosisProcessor
from flask import Flask,request

logger = logging.getLogger(__name__)

print("ç¨‹åºå¼€å§‹")
print(API_KEY)
# import warnings
# warnings.filterwarnings("ignore")

local = False

if local:
    # main_llm = ChatOpenAI(temperature=0.7, model=CHAT_MODEL, api_key=API_KEY, base_url=HOST + "/v1")
    # main_llm = ChatOllama(temperature=0.7, model=CHAT_MODEL, base_url=HOST + "/v1")
    main_llm = ChatOpenAI(temperature=0.7, model=CHAT_MODEL, api_key=API_KEY)
else:
    main_llm = ChatOpenAI(temperature=0.7, model="gpt-4o", api_key=API_KEY)

# ä¸Šä¸‹æ–‡è®°å¿†è®¾ç½®
# embedding_fn = OllamaEmbeddings(model=EMBEDDING_MODEL, base_url=HOST)
embedding_fn = OpenAIEmbeddings(model=EMBEDDING_MODEL, api_key=API_KEY)
sample_embedding = embedding_fn.embed_query("Sample text")
actual_dimension = len(sample_embedding)

index = faiss.IndexFlatL2(actual_dimension)
vectorstore = FAISS(embedding_function=embedding_fn.embed_query, index=index, docstore=InMemoryDocstore({}), index_to_docstore_id={})
retriever = vectorstore.as_retriever(search_kwargs=dict(k=3))
memory = VectorStoreRetrieverMemory(retriever=retriever)

implicit_memory_knowledge_base = implicit_memory.ImplicitMemorySystem()
explicit_memory_knowledge_base = explicit_memory.ExplicitMemorySystem()


def generate_session_id():
    return str(uuid.uuid4())

class JSONEncoder(json.JSONEncoder):
    def default(self, o):
        if isinstance(o, ObjectId):
            return str(o)
        return json.JSONEncoder.default(self, o)

class Graph_Knowledge_Retrieve(BaseTool):
    name: str = "graph_knowledge_retrieve"
    description: str = "æ­¤å·¥å…·ç”¨äºæ£€ç´¢ä¸ç‰¹å®šç–¾ç—…ç›¸å…³çš„çŸ¥è¯†å›¾è°±ï¼Œå¸®åŠ©ç”¨æˆ·è§£ç­”å…³äºç‰¹å®šç–¾ç—…çš„ç–‘æƒ‘ã€‚å½“ç”¨æˆ·æœ‰å…³äºç²¾ç¥ç–¾ç—…çš„ç–‘é—®æ—¶ï¼Œè°ƒç”¨è¯¥å·¥å…·ï¼Œå¦‚æœæœ‰è¿”å›åˆ™ç»“åˆè¿”å›å†…å®¹å’Œè‡ªå·±çš„çŸ¥è¯†å›å¤ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨è‡ªå·±çš„çŸ¥è¯†å›å¤"
    class ArgsSchema(BaseModel):
        query: str = Field(..., description="åŒ…å«ç‰¹å®šç–¾ç—…çš„å®ä½“å’Œå…³ç³»çš„æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼šæŠ‘éƒç—‡çš„æ²»ç–—æ–¹æ³•æœ‰å“ªäº›?")
    args_schema: Type[BaseModel] = ArgsSchema
    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        result = retrieve.run(query)
        return json.dumps({
            "tool_name": self.name,
            "tool_input": query,
            "tool_output": result
        })

class Web_Search(BaseTool):
    name: str = "web_search"
    description: str = f"æ­¤å·¥å…·ç”¨äºè·å–æœ€æ–°æ–°é—»å’Œä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·è·å–æœ€æ–°ä¿¡æ¯ï¼Œä½ çš„çŸ¥è¯†æœ€æ–°åˆ°2023å¹´11æœˆï¼Œè€Œä»Šå¤©æ˜¯{datetime.now().strftime('%Y-%m-%d')}ã€‚å½“ç”¨æˆ·çš„è¯·æ±‚æ˜æ˜¾è¦æ±‚éœ€è¦æœ€æ–°çš„ä¿¡æ¯æ”¯æ’‘æ—¶ï¼Œå¯ä»¥å°è¯•è°ƒç”¨è¯¥å·¥å…·ã€‚å¦åˆ™ï¼Œè¯·å¿½ç•¥ã€‚"
    class ArgsSchema(BaseModel):
        query: str = Field(..., description="éœ€è¦åœ¨äº’è”ç½‘ä¸Šæœç´¢çš„å®Œæ•´æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼šå…³äºæŠ‘éƒç—‡çš„æœ€æ–°æ–°é—»æœ‰ä»€ä¹ˆï¼Ÿ")
    args_schema: Type[BaseModel] = ArgsSchema

    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> Union[List[Dict], str]:
        result = asyncio.run(web_search.run(query))
        return json.dumps({
            "tool_name": self.name,
            "tool_input": query,
            "tool_output": result
        })

class Memory_Retrieve(BaseTool):
    name: str = "memory_retrieve"
    description: str = """æ­¤å·¥å…·ç”¨äºä»è®°å¿†ç³»ç»Ÿä¸­æ£€ç´¢ç”¨æˆ·ç›¸å…³è®°å¿†ã€‚ä½ éœ€è¦æ ¹æ®æŸ¥è¯¢å†…å®¹ï¼Œä»ä»¥ä¸‹ç±»åˆ«ä¸­é€‰æ‹©æœ€ç›¸å…³çš„ç±»åˆ«è¿›è¡Œæ£€ç´¢ï¼š
    æƒ…ç»ªä½“éªŒ: åŒ…æ‹¬å½“å‰æƒ…ç»ªçŠ¶æ€ã€æƒ…ç»ªå¼ºåº¦ã€æƒ…ç»ªå˜åŒ–ç­‰ç›´æ¥çš„æƒ…æ„Ÿä½“éªŒ
    è¡Œä¸ºæ¨¡å¼: åŒ…æ‹¬å®é™…çš„è¡Œä¸ºååº”ã€åº”å¯¹ç­–ç•¥ã€äººé™…äº’åŠ¨æ–¹å¼ç­‰å¯è§‚å¯Ÿçš„è¡Œä¸º
    è®¤çŸ¥ç‰¹å¾: åŒ…æ‹¬æ€ç»´æ–¹å¼ã€ä¿¡å¿µç³»ç»Ÿã€è®¤çŸ¥åå·®ç­‰æ€ç»´å±‚é¢çš„ç‰¹å¾
    å†å²ä¿¡æ¯: åŒ…æ‹¬åˆ›ä¼¤ç»å†ã€é‡è¦ç”Ÿæ´»äº‹ä»¶ã€æˆé•¿ç»å†ç­‰å†å²æ€§ä¿¡æ¯
    äººæ ¼ç‰¹è´¨: åŒ…æ‹¬ç¨³å®šçš„æ€§æ ¼ç‰¹å¾ã€ä¾æ‹æ–¹å¼ã€é˜²å¾¡æœºåˆ¶ç­‰
    äººå£å­¦ä¿¡æ¯: åŒ…æ‹¬åŸºæœ¬äººå£ç»Ÿè®¡å­¦ç‰¹å¾
    ä¸»è¯‰: åŒ…æ‹¬ä¸»è¦ç—‡çŠ¶å’Œä¸»è¯‰å†…å®¹
    ç°ç—…å²: åŒ…æ‹¬å½“å‰ç–¾ç—…çš„å‘å±•è¿‡ç¨‹
    ç”¨è¯å²: åŒ…æ‹¬ç”¨è¯æƒ…å†µå’Œè¯ç‰©ååº”
    ç‰©è´¨ä½¿ç”¨å²: åŒ…æ‹¬æˆç˜¾ç‰©è´¨çš„ä½¿ç”¨æƒ…å†µ
    å®¶æ—å²: åŒ…æ‹¬å®¶åº­ç—…å²å’Œå®¶åº­å…³ç³»
    ç¤¾ä¼šå²: åŒ…æ‹¬ç¤¾ä¼šåŠŸèƒ½å’Œç¤¾ä¼šæ”¯æŒ
    åˆ›ä¼¤å²: åŒ…æ‹¬é‡å¤§åˆ›ä¼¤ç»å†
    æ²»ç–—å²: åŒ…æ‹¬æ—¢å¾€æ²»ç–—ç»å†å’Œæ•ˆæœ
    """

    class ArgsSchema(BaseModel):
        categories: List[str] = Field(description="æ ¹æ®æŸ¥è¯¢å†…å®¹é€‰æ‹©çš„è®°å¿†ç±»åˆ«åˆ—è¡¨")
    args_schema: Type[BaseModel] = ArgsSchema

    def _run(self, categories: List[str], run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        memory_system = memory_retrieve.MemoryRetrievalSystem()
        raw_memories = memory_system.retrieve_memories_by_categories(user_id=user_id, categories=categories)
        memories = memory_system.parse_memory_result(raw_memories)
        return json.dumps({
            "tool_name": self.name,
            "tool_input": {
                "user_id": user_id,
                "categories": categories
            },
            "tool_output": memories
        }, ensure_ascii=False)

tools = [Graph_Knowledge_Retrieve(), Web_Search(), Memory_Retrieve()]
tool_executor = ToolExecutor(tools=tools)

functions = [convert_to_openai_function(t) for t in tools]
model = main_llm.bind_functions(functions)

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    session_id: str
    user_id: str
    start_time: datetime

def initialize_state(system_message: SystemMessage, user_id: str) -> AgentState:
    return {
        "messages": [system_message],
        "session_id": generate_session_id(),
        "user_id": user_id,
        "start_time": datetime.now()
    }

def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    function_call = last_message.additional_kwargs.get("function_call")
    if not function_call:
        return "end"
    elif function_call["name"] in ["graph_knowledge_retrieve", "web_search", "memory_retrieve"]:
        return "continue"
    else:
        return "end"

def call_model(state):
    messages = state["messages"]
    last_message = messages[-1]
    history = memory.load_memory_variables({"prompt": last_message.content})["history"]
    input_text = f"{messages[0].content}\n{history}\näººç±»: {last_message.content}\nåŠ©æ‰‹: "
    response = model.invoke(input_text)
    memory.save_context({"input": last_message.content}, {"output": response.content})
    return {"messages": [response]}

def call_tool(state):
    messages = state["messages"]
    last_message = messages[-1]
    action = ToolInvocation(
        tool=last_message.additional_kwargs["function_call"]["name"],
        tool_input=json.loads(
            last_message.additional_kwargs["function_call"]["arguments"]
        ),
    )
    response = tool_executor.invoke(action)
    function_message = FunctionMessage(content=response, name=action.tool)
    return {"messages": [function_message]}

async def handle_conversation(user_input: str, state: AgentState) -> Tuple[AgentState, str, Optional[Dict]]:
    response_messages = []
    tool_data = None
    human_message = HumanMessage(content=user_input)
    state["messages"].append(human_message)
    memory.save_context({"input": user_input}, {"output": ""})
    for output in app.stream(state):
        for key, value in output.items():
            if key == "__end__":
                continue
            if isinstance(value, dict) and "messages" in value:
                messages_list = value["messages"]
                for message in messages_list:
                    if isinstance(message, FunctionMessage):
                        try:
                            tool_data = json.loads(message.content)
                        except json.JSONDecodeError:
                            print(f"Warning: Unable to parse FunctionMessage content as JSON: {message.content}")
                            tool_data = {
                                "tool_name": message.name,
                                "tool_output": message.content
                            }

                        ai_input = f"ä»¥ä¸‹æ˜¯{tool_data['tool_name']}å·¥å…·è¿”å›çš„ç»“æœ: </START>{tool_data['tool_output']}</END>\nï¼Œè¯·é‡æ–°ç»„ç»‡åç»§ç»­ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œè®°ä½ï¼Œä½ ä¸éœ€è¦è¯´æ˜è¿™äº›ä¿¡æ¯æ˜¯æ¥è‡ªäºå“ªçš„ï¼Œä½ å¯ä»¥ä½œä¸ºè‡ªå·±çš„çŸ¥è¯†æ¥è¿ç”¨ã€‚"
                        ai_response = await model.ainvoke(ai_input)
                        response_messages.append(ai_response.content)
                    if isinstance(message, AIMessage):
                        response_messages.append(message.content)
    state["messages"] = state["messages"][:1]
    return state, "\n".join(response_messages), tool_data

workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("action", call_tool)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", END)
app = workflow.compile()

async def run_psy_predict(user_id, user_input):
    psy_pred = implicit_memory_knowledge_base.process_user_input(user_id, [user_input])
    print(Fore.BLUE + f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”> ||| éšå¼è®°å¿†æ¨æ–­: {psy_pred}" + Style.RESET_ALL)
    return psy_pred

async def run_memory_read(user_id, user_input):
    exp_pred = explicit_memory_knowledge_base.process_user_input(user_id, [user_input])
    print(Fore.BLUE + f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”> ||| æ˜¾å¼è®°å¿†æ¨æ–­: {exp_pred}" + Style.RESET_ALL)
    return exp_pred

async def run_handle_conversation(user_input: str, state: AgentState) -> Tuple[AgentState, str, Optional[Dict]]:
    new_state, response, tool_data = await handle_conversation(user_input, state)
    return new_state, response, tool_data

async def websocket_echo(websocket, path):
    async for message in websocket:
        print(f"Received: {message}")
        await websocket.send(message)
        print(f"Sent: {message}")

async def handle_websocket(websocket, path):
        global user_id
        state = None
        print("WebSocketè¿æ¥å·²å»ºç«‹ï¼Œç­‰å¾…ç”¨æˆ·æ•°æ®...")
        def choose_consultation_type(type_value):
            if type_value == 0:
                return main_system.main_prompt()

            consultation_types = {
                1: guided_conversation.clinical_psychological_consultation,
                2: guided_conversation.marriage_and_family_counseling,
                3: guided_conversation.child_and_adolescent_psychology,
                4: guided_conversation.career_counseling,
                5: guided_conversation.health_psychology,
                6: guided_conversation.addiction_counseling,
                7: guided_conversation.trauma_counseling
            }

            return consultation_types.get(type_value, main_system.main_prompt())

        def get_system_prompt(json_data):
            type_value = json_data.get('type', 0)
            return choose_consultation_type(type_value)

        try:
            print("WebSocketè¿æ¥å·²å»ºç«‹ï¼Œç­‰å¾…ç”¨æˆ·æ•°æ®...")
            while True:
                try:
                    data = await websocket.recv()
                    json_data = json.loads(data)
                    print(f"æ”¶åˆ°æ•°æ®: {json_data}")
                    logger.info(f"æ¥æ”¶åˆ°çš„æ•°æ® - ç”¨æˆ·ID: {json_data.get('user_id')}, é—®é¢˜: {json_data.get('question')}, ç±»å‹: {json_data.get('type')}")

                    user_id = json_data.get('user_id')
                    user_input = json_data.get('question')

                    if not user_id or user_input is None:
                        await websocket.send(json.dumps({"error": "æ— æ•ˆçš„æ•°æ®æ ¼å¼ã€‚ç¼ºå°‘user_idæˆ–questionã€‚"}))
                        continue

                    if user_input.lower() == "\\exit" or user_input == "\\ç»“æŸ":
                        logger.info(f"å¯¹è¯ç»“æŸ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")
                        await websocket.send(json.dumps({"message": f"å†è§ğŸ‘‹ {user_id}, æœŸå¾…æˆ‘ä»¬çš„ä¸‹æ¬¡è§é¢!ğŸ¥³"}))
                        break

                    logger.info(f"ç”¨æˆ·è¾“å…¥ - å†…å®¹: {user_input}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")

                    if state is None:
                        system_prompt = get_system_prompt(json_data)
                        system_message = SystemMessage(content=dedent(system_prompt))
                        state = initialize_state(system_message, user_id)

                    psy_pred, exp_pred = await asyncio.gather(
                        run_psy_predict(user_id, user_input),
                        run_memory_read(user_id, user_input)
                    )

                    state, response, tool_data = await run_handle_conversation(user_input, state)

                    response_data = {
                        "message": response,
                        "tool_data": tool_data,
                        "memory_data": {
                            "implicit_memory": psy_pred,
                            "explicit_memory": exp_pred
                        }
                    }

                    await websocket.send(json.dumps(response_data, cls=JSONEncoder))

                    logger.info(f"AIå“åº” - å†…å®¹é•¿åº¦: {len(response_data['message'])}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

                except asyncio.TimeoutError:
                    logger.warning(f"ç”¨æˆ·è¾“å…¥è¶…æ—¶ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")
                    await websocket.send(json.dumps({"message": "æ‚¨å¥½ï¼Œæ‚¨å·²ç»å¾ˆé•¿æ—¶é—´æ²¡æœ‰å‘é€æ¶ˆæ¯äº†ã€‚å¦‚æœæ‚¨è¿˜åœ¨çº¿ï¼Œè¯·å›å¤ä»»æ„æ¶ˆæ¯ã€‚"}))

        except websockets.exceptions.ConnectionClosedOK:
            print(f"WebSocket connection closed normally for user: {user_id}")
            logger.info(f"WebSocketè¿æ¥æ­£å¸¸å…³é—­ - ç”¨æˆ·ID: {user_id}")
        except websockets.exceptions.ConnectionClosedError as e:
            print(f"WebSocket connection closed with error for user: {user_id}. Error: {e}")
            logger.error(f"WebSocketè¿æ¥å¼‚å¸¸å…³é—­ - ç”¨æˆ·ID: {user_id}, é”™è¯¯: {str(e)}")
        except Exception as e:
            print(f"Unexpected error in WebSocket communication: {str(e)}")
            logger.error(f"WebSocketé€šä¿¡æœªé¢„æœŸçš„é”™è¯¯ - ç”¨æˆ·ID: {user_id}, é”™è¯¯: {str(e)}")
            import traceback
            print(traceback.format_exc())
        finally:
            print(f"WebSocket connection closed for user: {user_id}")
            logger.info(f"WebSocketè¿æ¥å·²å…³é—­ - ç”¨æˆ·ID: {user_id}")

async def start_websocket_server():
    print("Starting WebSocket server on ws://localhost:8763")
    try:
        server = await websockets.serve(handle_websocket, "0.0.0.0", 8763)
        print("WebSocket server started on ws://localhost:8765")
        await server.wait_closed()
    except Exception as e:
        print(f"Error starting WebSocket server: {str(e)}")

async def handle_console_interaction():
    global user_id
    print("\n\nè¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·åæˆ–I1D: ")
    user_id = await asyncio.get_event_loop().run_in_executor(None, input)

    guided = await asyncio.get_event_loop().run_in_executor(None, lambda: input("æ˜¯å¦éœ€è¦è¿›è¡Œå¼•å¯¼æ€§å¯¹è¯æµ‹è¯•ï¼Ÿï¼ˆYes/Noï¼‰: "))
    guided = guided.lower() == "yes"

    if guided:
        system_prompt = guided_conversation.choose_consultation_type()
    else:
        system_prompt = main_system.main_prompt()

    system_message = SystemMessage(content=dedent(system_prompt))
    state = initialize_state(system_message, user_id)

    logger.info(f"æ–°å¯¹è¯å¼€å§‹ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

    print("\n--------------------------------------â¤ï¸æ¬¢è¿æ¥åˆ°å¿ƒç†æ²»ç–—å®¤â¤ï¸--------------------------------------\n")
    print(f"ä½ å¥½ {user_id}! æˆ‘æ˜¯EiğŸ™‚, æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—?\n")

    while True:
        user_input = await asyncio.get_event_loop().run_in_executor(None, input, ">>: ")
        if user_input.lower() == "\\exit" or user_input == "\\ç»“æŸ":
            logger.info(f"å¯¹è¯ç»“æŸ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")
            print(f"å†è§ğŸ‘‹ {user_id}, æœŸå¾…æˆ‘ä»¬çš„ä¸‹æ¬¡è§é¢!ğŸ¥³")
            break

        logger.info(f"ç”¨æˆ·è¾“å…¥ - å†…å®¹: {user_input}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

        psy_pred, exp_pred = await asyncio.gather(
            run_psy_predict(user_id, user_input),
            run_memory_read(user_id, user_input)
        )

        state, response, tool_data = await run_handle_conversation(user_input, state)
        print("\nEi: ", response)

        if tool_data:
            print("\nå·¥å…·è°ƒç”¨ä¿¡æ¯:")
            print(f"å·¥å…·åç§°: {tool_data['tool_name']}")
            print(f"å·¥å…·è¾“å…¥: {tool_data['tool_input']}")
            print(f"å·¥å…·è¾“å‡º: {tool_data['tool_output']}")

        print("\nè®°å¿†æ•°æ®:")
        print(f"éšå¼è®°å¿†: {psy_pred}")
        print(f"æ˜¾å¼è®°å¿†: {exp_pred}")

        logger.info(f"AIå“åº” - å†…å®¹é•¿åº¦: {len(response)}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")
        print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”>")

async def handle_console_interactio1():
    print("\n\nè¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·åæˆ–I1D:1111 ")

async def main_loop():
    print("ç¨‹åºå¼€å§‹1")
    # å¦‚æœä½ æƒ³ä½¿ç”¨æ—¥å¿—ï¼ˆElasticsearch æˆ–æ–‡ä»¶ï¼‰
    _, _ = setup_logging()
    # æˆ–è€…ï¼Œå¦‚æœä½ æƒ³å®Œå…¨ç¦ç”¨æ—¥å¿—
    # _, _ = disable_logging()
    logger = logging.getLogger(__name__)
    try:
        print("ç¨‹åºå¼€å§‹2")
        websocket_server = asyncio.create_task(start_websocket_server())
        console_interaction = asyncio.create_task(handle_console_interaction())
        await asyncio.gather(websocket_server,console_interaction)
    except Exception as e:
        print(f"ä¸»å¾ªç¯é”™è¯¯: {str(e)}")
        # logger.error(f"ä¸»å¾ªç¯é”™è¯¯: {str(e)}")
    finally:
        print("ç¨‹åºç»“æŸ")
        # logger.info("ç¨‹åºç»“æŸ")

app = Flask(__name__)
@app.route('/apiv1/diagnosis/processor', strict_slashes=False, methods=['POST'])
def processor_main():
    try:
        fields = request.get_json(force=True)
        token = request.headers.get('X-Ivanka-Token')
        if not token:
            return json.dumps("TOKENä¸ºç©º",ensure_ascii=False)
        # print("fields")
        # print(fields)
        processor = MedicalDiagnosisProcessor()
        # test_input = {
        #     "è¿‡æ•å²": "è¯ç‰©è¿‡æ•å²ï¼šæœªå‘ç°ï¼›é£Ÿç‰©è¿‡æ•å²ï¼šå¦è®¤",
        #     "ä¸ªäººå²": "å¦è®¤é•¿æœŸæ¥è§¦æœ‰æ¯’æœ‰å®³ç‰©è´¨å²ï¼Œå¦è®¤ä¸¥é‡åˆ›ä¼¤å²ï¼Œå¦è®¤é•¿æœŸå§åºŠå²ï¼Œå¦è®¤æ‰‹æœ¯å²ã€‚",
        #     "å©šè‚²å²": "å·²å©šï¼Œå·²è‚²ä¸€å­",
        #     "å®¶æ—å²": "çˆ¶æ¯å¥åœ¨ï¼Œå¦è®¤å®¶æ—é—ä¼ ç—…å²",
        #     "è¯Šç–—ç»è¿‡": ""
        # }
        result = processor.process_diagnosis(json.dumps(fields))
        resp = processor.output_format(raw_results=result)
        return json.dumps(resp,ensure_ascii=False)
        #print("\nè¯Šæ–­ç»“æœï¼š",resp)
        # if result:
        #     print("\nè¯Šæ–­ç»“æœï¼š")
        #     for i, diagnosis in enumerate(result.è¯Šæ–­ç»“æœ, 1):
        #         print(f"\nå¯èƒ½æ€§ {i}:")
        #         print(f"ç—…ç—‡: {diagnosis.ç—…ç—‡}")
        #         print(f"ç½®ä¿¡åº¦: {diagnosis.ç½®ä¿¡åº¦}")
        #         print(f"ç†ç”±: {diagnosis.ç†ç”±}")
        # else:
        #     print("\næœªèƒ½ç”Ÿæˆè¯Šæ–­ç»“æœ")
    except Exception as e:
        logger.error(f"Error in main: {str(e)}", exc_info=True)
        print(f"\nç¨‹åºæ‰§è¡Œå‡ºé”™: {str(e)}")
if __name__ == "__main__":
    # try:
    #     from load_config import WEB_SOCKET_PORT
    #     _, _ = setup_logging()
    #     logger = logging.getLogger(__name__)
    #     server = websockets.serve(handle_websocket, "0.0.0.0", WEB_SOCKET_PORT)
    #     asyncio.get_event_loop().run_until_complete(server)
    #     asyncio.get_event_loop().run_forever()
    # except Exception as e:
    #     print(f"Error starting WebSocket server: {str(e)}")
    #     logger.error(f"Error starting WebSocket server: {str(e)}")
       # asyncio.run(main_loop())
        # asyncio.run(processor_main())
        app.run(debug=False, host='0.0.0.0', port=8763)
