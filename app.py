import rootutils
rootutils.setup_root(__file__, indicator=".project-root", pythonpath=True)

import os
import asyncio
import json
import operator
import uuid
from textwrap import dedent
from datetime import datetime
from typing import Optional, Union, List, Dict, Type, TypedDict, Annotated, Sequence, Tuple

import faiss
import websockets
from colorama import Fore, Style
from sentence_transformers import SentenceTransformer
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_community.vectorstores.faiss import FAISS
from langchain_core.callbacks import CallbackManagerForToolRun
from langchain_core.messages import HumanMessage, SystemMessage, BaseMessage, FunctionMessage, AIMessage
from langchain_core.pydantic_v1 import BaseModel, Field
from langchain_core.tools import BaseTool
from langchain_core.utils.function_calling import convert_to_openai_function
from langchain_openai import ChatOpenAI
from langchain.memory import VectorStoreRetrieverMemory
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode, ToolInvocation, ToolExecutor

from tools import summarize, web_search
from rag.knowledge_graph import retrieve
from memory import explicit_memory, implicit_memory, memory_retrieve
from rag.historical_exp.calculate_similarity import PatientDiagnosisAPI
from prompts import guided_conversation, main_system

from load_config import GPT4O, OPENAI_API_KEY
from logging_config import setup_logging, disable_logging
import logging

logger = logging.getLogger(__name__)

import warnings
warnings.filterwarnings("ignore")

main_llm = ChatOpenAI(temperature=0.7, model=GPT4O, api_key=OPENAI_API_KEY)

class LocalEmbeddings:
    def __init__(self, model_name='all-MiniLM-L6-v2'):
        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
    def __call__(self, text):
        return self.embed_documents([text])[0]
    def embed_documents(self, texts):
        return self.model.encode(texts)
    def embed_query(self, text):
        return self.model.encode([text])[0]

# ä¸Šä¸‹æ–‡è®°å¿†è®¾ç½®
embeddings = LocalEmbeddings()
dimension = embeddings.dimension
index = faiss.IndexFlatL2(dimension)
vectorstore = FAISS(embedding_function=embeddings, index=index, docstore=InMemoryDocstore({}), index_to_docstore_id={})
retriever = vectorstore.as_retriever(search_kwargs=dict(k=3))
memory = VectorStoreRetrieverMemory(retriever=retriever)

def generate_session_id():
    return str(uuid.uuid4())

class Graph_Knowledge_Retrieve(BaseTool):
    name: str = "graph_knowledge_retrieve"
    description: str = "æ­¤å·¥å…·ç”¨äºæ£€ç´¢ä¸ç‰¹å®šç–¾ç—…ç›¸å…³çš„çŸ¥è¯†å›¾è°±ï¼Œå¸®åŠ©ç”¨æˆ·è§£ç­”å…³äºç‰¹å®šç–¾ç—…çš„ç–‘æƒ‘ã€‚å½“ç”¨æˆ·æœ‰å…³äºç²¾ç¥ç–¾ç—…çš„ç–‘é—®æ—¶ï¼Œè°ƒç”¨è¯¥å·¥å…·ï¼Œå¦‚æœæœ‰è¿”å›åˆ™ç»“åˆè¿”å›å†…å®¹å’Œè‡ªå·±çš„çŸ¥è¯†å›å¤ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ä½¿ç”¨è‡ªå·±çš„çŸ¥è¯†å›å¤"
    class ArgsSchema(BaseModel):
        query: str = Field(..., description="åŒ…å«ç‰¹å®šç–¾ç—…çš„å®ä½“å’Œå…³ç³»çš„æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼šæŠ‘éƒç—‡çš„æ²»ç–—æ–¹æ³•æœ‰å“ªäº›?")
    args_schema: Type[BaseModel] = ArgsSchema
    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> str:
        result = asyncio.run(retrieve.run(query))
        return json.dumps({
            "tool_name": self.name,
            "tool_input": query,
            "tool_output": result
        })

class Web_Search(BaseTool):
    name: str = "web_search"
    description: str = "æ­¤å·¥å…·ç”¨äºè·å–æœ€æ–°æ–°é—»å’Œä¿¡æ¯ï¼Œå¸®åŠ©ç”¨æˆ·è·å–æœ€æ–°ä¿¡æ¯ã€‚å½“ç”¨æˆ·çš„è¯·æ±‚æ˜æ˜¾è¦æ±‚éœ€è¦æœ€æ–°çš„ä¿¡æ¯æ”¯æ’‘æ—¶ï¼Œå¯ä»¥å°è¯•è°ƒç”¨è¯¥å·¥å…·ã€‚å¦åˆ™ï¼Œè¯·å¿½ç•¥ã€‚"
    class ArgsSchema(BaseModel):
        query: str = Field(..., description="éœ€è¦åœ¨äº’è”ç½‘ä¸Šæœç´¢çš„å®Œæ•´æŸ¥è¯¢ã€‚ä¾‹å¦‚ï¼šå…³äºæŠ‘éƒç—‡çš„æœ€æ–°æ–°é—»æœ‰ä»€ä¹ˆï¼Ÿ")
    args_schema: Type[BaseModel] = ArgsSchema
    def _run(self, query: str, run_manager: Optional[CallbackManagerForToolRun] = None) -> Union[List[Dict], str]:
        result = asyncio.run(web_search.run(query))
        return json.dumps({
            "tool_name": self.name,
            "tool_input": query,
            "tool_output": result
        })

class Memory_Retrieve(BaseTool):
    name: str = "memory_retrieve"
    description: str = "æ­¤å·¥å…·ç”¨äºä»è®°å¿†ä¸­æ£€ç´¢å…³äºç”¨æˆ·çš„è®°å¿†ï¼ŒåŒ…æ‹¬ä¸ªäººå±æ€§ï¼Œç¤¾äº¤å…³ç³»ï¼Œå·¥ä½œçŠ¶æ€ï¼Œå¿ƒæ™ºçŠ¶æ€ç­‰ç­‰ã€‚å½“ä½ ä¸çŸ¥é“ç”¨æˆ·çš„ä¸€äº›ä¿¡æ¯æ—¶ï¼Œè°ƒç”¨è¯¥å·¥å…·ã€‚"
    class ArgsSchema(BaseModel):
        explicit_memory_query: Optional[str] = Field(None, description="ä½ éœ€è¦æ£€ç´¢çš„æ˜¾å¼è®°å¿†ã€‚æ˜¾å¼è®°å¿†æ˜¯ç”¨æˆ·çš„ä¸ªäººå±æ€§ã€å®¶åº­å±æ€§å’Œç¤¾ä¼šå±æ€§ç›¸å…³çš„è®°å¿†ã€‚ä¾‹å¦‚ï¼š1.ä»–çš„å¹´é¾„æ˜¯å¤šå°‘ï¼Ÿ2.ä»–æœ€è¿‘çš„å·¥ä½œæ˜¯ä»€ä¹ˆï¼Ÿ")
        implicit_memory_query: Optional[str] = Field(None, description="ä½ éœ€è¦æ£€ç´¢çš„éšå¼è®°å¿†ã€‚éšå¼è®°å¿†æ˜¯ç”¨æˆ·çš„å¿ƒç†çŠ¶æ€ã€å¿ƒæ™ºèƒ½åŠ›çš„å†å²æ¨è®ºã€‚ä¾‹å¦‚ï¼š1. ä»–æœ€è¿‘çš„å¿ƒç†çŠ¶æ€æ˜¯ä»€ä¹ˆï¼Ÿ2. ä»–å…·æœ‰å¤šé‡äººæ ¼å—ï¼Ÿ")
    args_schema: Type[BaseModel] = ArgsSchema
    def _run(self, explicit_memory_query: Optional[str] = None, implicit_memory_query: Optional[str] = None, run_manager: Optional[CallbackManagerForToolRun] = None) -> Union[List[str], str]:
        result = memory_retrieve.run(explicit_memory_query or "", implicit_memory_query or "", user_id)
        return json.dumps({
            "tool_name": self.name,
            "tool_input": {
                "explicit_memory_query": explicit_memory_query,
                "implicit_memory_query": implicit_memory_query
            },
            "tool_output": result
        })

tools = [Graph_Knowledge_Retrieve(), Web_Search(), Memory_Retrieve()]
tool_executor = ToolExecutor(tools=tools)

functions = [convert_to_openai_function(t) for t in tools]
model = main_llm.bind_functions(functions)

class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], operator.add]
    session_id: str
    user_id: str
    start_time: datetime

def initialize_state(system_message: SystemMessage, user_id: str) -> AgentState:
    return {
        "messages": [system_message],
        "session_id": generate_session_id(),
        "user_id": user_id,
        "start_time": datetime.now()
    }

def should_continue(state):
    messages = state["messages"]
    last_message = messages[-1]
    function_call = last_message.additional_kwargs.get("function_call")
    if not function_call:
        return "end"
    elif function_call["name"] in ["graph_knowledge_retrieve", "web_search", "memory_retrieve"]:
        return "continue"
    else:
        return "end"

def call_model(state):
    messages = state["messages"]
    last_message = messages[-1]
    history = memory.load_memory_variables({"prompt": last_message.content})["history"]
    input_text = f"{messages[0].content}\n{history}\näººç±»: {last_message.content}\nåŠ©æ‰‹: "
    response = model.invoke(input_text)
    memory.save_context({"input": last_message.content}, {"output": response.content})
    return {"messages": [response]}

def call_tool(state):
    messages = state["messages"]
    last_message = messages[-1]
    action = ToolInvocation(
        tool=last_message.additional_kwargs["function_call"]["name"],
        tool_input=json.loads(
            last_message.additional_kwargs["function_call"]["arguments"]
        ),
    )
    response = tool_executor.invoke(action)
    function_message = FunctionMessage(content=response, name=action.tool)
    return {"messages": [function_message]}

async def handle_conversation(user_input: str, state: AgentState) -> Tuple[AgentState, str, Optional[Dict]]:
    response_messages = []
    tool_data = None
    human_message = HumanMessage(content=user_input)
    state["messages"].append(human_message)
    memory.save_context({"input": user_input}, {"output": ""})
    for output in app.stream(state):
        for key, value in output.items():
            if key == "__end__":
                continue
            if isinstance(value, dict) and "messages" in value:
                messages_list = value["messages"]
                for message in messages_list:
                    if isinstance(message, FunctionMessage):
                        try:
                            tool_data = json.loads(message.content)
                        except json.JSONDecodeError:
                            print(f"Warning: Unable to parse FunctionMessage content as JSON: {message.content}")
                            tool_data = {
                                "tool_name": message.name,
                                "tool_output": message.content
                            }
                        
                        ai_input = f"ä»¥ä¸‹æ˜¯{tool_data['tool_name']}å·¥å…·è¿”å›çš„ç»“æœ: </START>{tool_data['tool_output']}</END>\nï¼Œè¯·é‡æ–°ç»„ç»‡åç»§ç»­ä¸ç”¨æˆ·è¿›è¡Œå¯¹è¯ï¼Œè®°ä½ï¼Œä½ ä¸éœ€è¦è¯´æ˜è¿™äº›ä¿¡æ¯æ˜¯æ¥è‡ªäºå“ªçš„ï¼Œä½ å¯ä»¥ä½œä¸ºè‡ªå·±çš„çŸ¥è¯†æ¥è¿ç”¨ã€‚"
                        ai_response = await model.ainvoke(ai_input)
                        response_messages.append(ai_response.content)
                    if isinstance(message, AIMessage):
                        response_messages.append(message.content)
    state["messages"] = state["messages"][:1]
    return state, "\n".join(response_messages), tool_data

workflow = StateGraph(AgentState)
workflow.add_node("agent", call_model)
workflow.add_node("action", call_tool)
workflow.set_entry_point("agent")
workflow.add_conditional_edges(
    "agent",
    should_continue,
    {
        "continue": "action",
        "end": END,
    },
)
workflow.add_edge("action", END)
app = workflow.compile()

async def run_psy_predict(user_id, user_input):
    psy_pred = implicit_memory.infer_mental_state(user_id, user_input)
    print(Fore.BLUE + f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”> ||| éšå¼è®°å¿†æ¨æ–­: {psy_pred}" + Style.RESET_ALL)
    return psy_pred

async def run_memory_read(user_id, user_input):
    exp_pred = explicit_memory.record_patient_info(user_id, user_input)
    print(Fore.BLUE + f"â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”> ||| æ˜¾å¼è®°å¿†æ¨æ–­: {exp_pred}" + Style.RESET_ALL)
    return exp_pred

async def run_handle_conversation(user_input: str, state: AgentState) -> Tuple[AgentState, str, Optional[Dict]]:
    new_state, response, tool_data = await handle_conversation(user_input, state)
    return new_state, response, tool_data

async def handle_websocket(websocket, path):
    global user_id
    state = None
    
    def choose_consultation_type(type_value):
        if type_value == 0:
            return main_system.main_prompt()
        
        consultation_types = {
            1: guided_conversation.clinical_psychological_consultation,
            2: guided_conversation.marriage_and_family_counseling,
            3: guided_conversation.child_and_adolescent_psychology,
            4: guided_conversation.career_counseling,
            5: guided_conversation.health_psychology,
            6: guided_conversation.addiction_counseling,
            7: guided_conversation.trauma_counseling
        }
        
        return consultation_types.get(type_value, main_system.main_prompt())

    def get_system_prompt(json_data):
        type_value = json_data.get('type', 0)
        return choose_consultation_type(type_value)

    try:
        print("WebSocketè¿æ¥å·²å»ºç«‹ï¼Œç­‰å¾…ç”¨æˆ·æ•°æ®...")
        while True:
            try:
                data = await asyncio.wait_for(websocket.recv(), timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                json_data = json.loads(data)
                print(f"æ”¶åˆ°æ•°æ®: {json_data}")
                logger.info(f"æ¥æ”¶åˆ°çš„æ•°æ® - ç”¨æˆ·ID: {json_data.get('user_id')}, é—®é¢˜: {json_data.get('question')}, ç±»å‹: {json_data.get('type')}")
                
                user_id = json_data.get('user_id')
                user_input = json_data.get('question')

                if not user_id or user_input is None:
                    await websocket.send(json.dumps({"error": "æ— æ•ˆçš„æ•°æ®æ ¼å¼ã€‚ç¼ºå°‘user_idæˆ–questionã€‚"}))
                    continue

                if user_input.lower() == "\\exit" or user_input == "\\ç»“æŸ":
                    logger.info(f"å¯¹è¯ç»“æŸ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")
                    await websocket.send(json.dumps({"message": f"å†è§ğŸ‘‹ {user_id}, æœŸå¾…æˆ‘ä»¬çš„ä¸‹æ¬¡è§é¢!ğŸ¥³"}))
                    break

                logger.info(f"ç”¨æˆ·è¾“å…¥ - å†…å®¹: {user_input}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")

                if state is None:
                    system_prompt = get_system_prompt(json_data)
                    system_message = SystemMessage(content=dedent(system_prompt))
                    state = initialize_state(system_message, user_id)
                    logger.info(f"æ–°å¯¹è¯å¼€å§‹ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

                # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Šå‘½ä»¤
                if user_input.strip().startswith("è¯·å¯¹ç”¨æˆ·ç—…ä¾‹ä¿¡æ¯è¿›è¡Œæ‘˜è¦") or user_input.strip().startswith("è¯·ä½ æ ¹æ®ä½é™¢å·ä¸º"):
                    response_data = await handle_special_commands(user_input, user_id, state['session_id'], websocket)
                else:
                    psy_pred, exp_pred = await asyncio.gather(
                        run_psy_predict(user_id, user_input),
                        run_memory_read(user_id, user_input)
                    )

                    state, response, tool_data = await run_handle_conversation(user_input, state)
                    
                    response_data = {
                        "message": response,
                        "tool_data": tool_data,
                        "memory_data": {
                            "implicit_memory": psy_pred,
                            "explicit_memory": exp_pred
                        }
                    }

                    await websocket.send(json.dumps(response_data))
                
                logger.info(f"AIå“åº” - å†…å®¹é•¿åº¦: {len(response_data['message'])}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

            except asyncio.TimeoutError:
                logger.warning(f"ç”¨æˆ·è¾“å…¥è¶…æ—¶ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id'] if state else 'N/A'}")
                await websocket.send(json.dumps({"message": "æ‚¨å¥½ï¼Œæ‚¨å·²ç»å¾ˆé•¿æ—¶é—´æ²¡æœ‰å‘é€æ¶ˆæ¯äº†ã€‚å¦‚æœæ‚¨è¿˜åœ¨çº¿ï¼Œè¯·å›å¤ä»»æ„æ¶ˆæ¯ã€‚"}))

    except websockets.exceptions.ConnectionClosedOK:
        print(f"WebSocket connection closed normally for user: {user_id}")
        logger.info(f"WebSocketè¿æ¥æ­£å¸¸å…³é—­ - ç”¨æˆ·ID: {user_id}")
    except websockets.exceptions.ConnectionClosedError as e:
        print(f"WebSocket connection closed with error for user: {user_id}. Error: {e}")
        logger.error(f"WebSocketè¿æ¥å¼‚å¸¸å…³é—­ - ç”¨æˆ·ID: {user_id}, é”™è¯¯: {str(e)}")
    except Exception as e:
        print(f"Unexpected error in WebSocket communication: {str(e)}")
        logger.error(f"WebSocketé€šä¿¡æœªé¢„æœŸçš„é”™è¯¯ - ç”¨æˆ·ID: {user_id}, é”™è¯¯: {str(e)}")
        import traceback
        print(traceback.format_exc())
    finally:
        print(f"WebSocket connection closed for user: {user_id}")
        logger.info(f"WebSocketè¿æ¥å·²å…³é—­ - ç”¨æˆ·ID: {user_id}")

async def start_websocket_server():
    server = await websockets.serve(handle_websocket, "localhost", 8765)
    print("WebSocket server started on ws://localhost:8765")
    await server.wait_closed()

async def handle_console_interaction():
    global user_id
    print("\n\nè¯·è¾“å…¥æ‚¨çš„ç”¨æˆ·åæˆ–ID: ")
    user_id = await asyncio.get_event_loop().run_in_executor(None, input)
    
    guided = await asyncio.get_event_loop().run_in_executor(None, lambda: input("æ˜¯å¦éœ€è¦è¿›è¡Œå¼•å¯¼æ€§å¯¹è¯æµ‹è¯•ï¼Ÿï¼ˆYes/Noï¼‰: "))
    guided = guided.lower() == "yes"
    
    if guided:
        system_prompt = guided_conversation.choose_consultation_type()
    else:
        system_prompt = main_system.main_prompt()

    system_message = SystemMessage(content=dedent(system_prompt))
    state = initialize_state(system_message, user_id)
    
    logger.info(f"æ–°å¯¹è¯å¼€å§‹ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

    print("\n--------------------------------------â¤ï¸æ¬¢è¿æ¥åˆ°å¿ƒç†æ²»ç–—å®¤â¤ï¸--------------------------------------\n")
    print(f"ä½ å¥½ {user_id}! æˆ‘æ˜¯EiğŸ™‚, æœ‰ä»€ä¹ˆæˆ‘å¯ä»¥å¸®åŠ©ä½ çš„å—?\n")

    while True:
        user_input = await asyncio.get_event_loop().run_in_executor(None, input, ">>: ")
        if user_input.lower() == "\\exit" or user_input == "\\ç»“æŸ":
            logger.info(f"å¯¹è¯ç»“æŸ - ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")
            print(f"å†è§ğŸ‘‹ {user_id}, æœŸå¾…æˆ‘ä»¬çš„ä¸‹æ¬¡è§é¢!ğŸ¥³")
            break

        logger.info(f"ç”¨æˆ·è¾“å…¥ - å†…å®¹: {user_input}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

        if user_input.strip().startswith("è¯·å¯¹ç”¨æˆ·ç—…ä¾‹ä¿¡æ¯è¿›è¡Œæ‘˜è¦") or user_input.strip().startswith("è¯·ä½ æ ¹æ®ä½é™¢å·ä¸º"):
            response_data = await handle_special_commands(user_input, user_id, state['session_id'])
            print("\nEi: ", response_data['message'])
            if 'tool_data' in response_data:
                print("\nå·¥å…·è°ƒç”¨ä¿¡æ¯:")
                print(f"å·¥å…·åç§°: {response_data['tool_data']['tool_name']}")
                print(f"å·¥å…·è¾“å…¥: {response_data['tool_data']['tool_input']}")
                print(f"å·¥å…·è¾“å‡º: {response_data['tool_data']['tool_output']}")
        else:
            psy_pred, exp_pred = await asyncio.gather(
                run_psy_predict(user_id, user_input),
                run_memory_read(user_id, user_input)
            )

            state, response, tool_data = await run_handle_conversation(user_input, state)
            print("\nEi: ", response)
            
            if tool_data:
                print("\nå·¥å…·è°ƒç”¨ä¿¡æ¯:")
                print(f"å·¥å…·åç§°: {tool_data['tool_name']}")
                print(f"å·¥å…·è¾“å…¥: {tool_data['tool_input']}")
                print(f"å·¥å…·è¾“å‡º: {tool_data['tool_output']}")
            
            print("\nè®°å¿†æ•°æ®:")
            print(f"éšå¼è®°å¿†: {psy_pred}")
            print(f"æ˜¾å¼è®°å¿†: {exp_pred}")
            
            logger.info(f"AIå“åº” - å†…å®¹é•¿åº¦: {len(response)}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {state['session_id']}")

        print("â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”>")

async def handle_special_commands(user_input, user_id, session_id, websocket=None):
    response_data = {}
    if user_input.strip().startswith("è¯·å¯¹ç”¨æˆ·ç—…ä¾‹ä¿¡æ¯è¿›è¡Œæ‘˜è¦"):
        file_path = "./data/hpi.txt"
        tool_name = "summarize"
        tool_input = {"file_path": file_path}
        if os.path.exists(file_path):
            try:
                summarize_prompt = summarize.run(file_path)
                summarize_content = model.invoke(summarize_prompt)
                ai_output = summarize_content.content
                tool_output = summarize_content.content
                logger.info(f"æ–‡ä»¶æ€»ç»“å®Œæˆ - æ–‡ä»¶: {file_path}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
            except Exception as e:
                error_msg = f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}"
                tool_output = error_msg
                logger.error(f"æ–‡ä»¶å¤„ç†é”™è¯¯ - æ–‡ä»¶: {file_path}, é”™è¯¯: {error_msg}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
        else:
            tool_output = "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ - æ–‡ä»¶: {file_path}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
    
    elif user_input.strip().startswith("è¯·ä½ æ ¹æ®ä½é™¢å·ä¸º"):
        json_file_path = "./data/diagnose.json"
        tool_name = "diagnose"
        tool_input = {"file_path": json_file_path}
        historical_exp_api = PatientDiagnosisAPI()
        if os.path.exists(json_file_path):
            try:
                with open(json_file_path, 'r', encoding='utf-8') as json_file:
                    json_input = json.load(json_file)
                
                vector_results = historical_exp_api.process_query(json.dumps(json_input))
                diagnosis_prompt = main_system.diagnosis_prompt(json_input=json_input, vector_results=vector_results)
                diagnosis = model.invoke(diagnosis_prompt)
                ai_output = diagnosis.content
                tool_output = vector_results
                logger.info(f"è¯Šæ–­å®Œæˆ - æ–‡ä»¶: {json_file_path}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
            except Exception as e:
                error_msg = f"å¤„ç†JSONæ–‡ä»¶æˆ–è¿›è¡Œè¯Šæ–­æ—¶å‡ºé”™: {str(e)}"
                tool_output = error_msg
                logger.error(f"è¯Šæ–­é”™è¯¯ - æ–‡ä»¶: {json_file_path}, é”™è¯¯: {error_msg}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
        else:
            tool_output = "æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚"
            logger.warning(f"è¯Šæ–­æ–‡ä»¶ä¸å­˜åœ¨ - æ–‡ä»¶: {json_file_path}, ç”¨æˆ·ID: {user_id}, ä¼šè¯ID: {session_id}")
    
    else:
        tool_name = "unknown_command"
        tool_input = {"command": user_input}
        tool_output = "æœªçŸ¥å‘½ä»¤"

    response_data = {
        "message": ai_output,
        "tool_data": {
            "tool_name": tool_name,
            "tool_input": tool_input,
            "tool_output": tool_output
        }
    }

    if websocket:
        await websocket.send(json.dumps(response_data))

    return response_data

async def main_loop():
    # å¦‚æœä½ æƒ³ä½¿ç”¨æ—¥å¿—ï¼ˆElasticsearch æˆ–æ–‡ä»¶ï¼‰
    # _, _ = setup_logging()
    
    # æˆ–è€…ï¼Œå¦‚æœä½ æƒ³å®Œå…¨ç¦ç”¨æ—¥å¿—
    _, _ = disable_logging()

    logger = logging.getLogger(__name__)

    try:
        websocket_server = asyncio.create_task(start_websocket_server())
        console_interaction = asyncio.create_task(handle_console_interaction())
        await asyncio.gather(websocket_server, console_interaction)
    except Exception as e:
        logger.error(f"ä¸»å¾ªç¯é”™è¯¯: {str(e)}")
    finally:
        logger.info("ç¨‹åºç»“æŸ")

if __name__ == "__main__":
    asyncio.run(main_loop())